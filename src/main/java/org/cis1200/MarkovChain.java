package org.cis1200;

import java.util.*;

/**
 * A Markov Chain is a data structure that tracks the frequency with which one
 * token follows another token in a collection of sequences. (Recall that a
 * "token" is a nonempty String produced by TweetParser containing either a
 * word or punctuation.)
 *
 * This project uses a Markov Chain to model tweets by gathering the frequency
 * information from a Twitter feed. The MarkovChain to generates "plausible"
 * tweets by constructing a random walk through the chain according to the
 * frequencies.
 * Please see the homework instructions for more information on Markov Chains.
 * <p>
 * <b>TRAINING:</b>
 * <p>
 * An ILLUSTRATIVE EXAMPLE (see the corresponding test cases throughout the
 * project files). Suppose we want to train the MarkovChain on these two
 * {@code String}s that represent tweets:
 * <p>
 * {@code "a table and a chair"} and
 * <p>
 * {@code "a banana! and a banana?"}
 * <p>
 * We first <i>parse</i> these two tweets into sequences of {@code String}
 * <i>tokens</i> that represent the individual words and punctuation marks
 * of the tweet. (See {@link TweetParser#parseAndCleanTweet(String)}.)
 * For these two tweets, we get the lists below. (Note how
 * the punctuation is separated as individual tokens.)
 * <br>
 * {@code ["a", "table", "and", "a", "chair"]}
 * <br>
 * {@code ["a", "banana", "!", "and", "a", "banana", "?"]}
 * </p>
 * <p>
 * The {@code MarkovChain} that results from this training data maps each
 * observed
 * token to a {@code ProbabilityDistribution} that is based on the recorded
 * occurrences
 * of bigrams (adjacent tokens) in the data. The {@code MarkovChain} also
 * computes a {@code ProbabilityDistribution} that contains the frequencies with
 * which words
 * start the tweets of the training data.
 * <p>
 * If we print the Markov Chain resulting from the training data above, we see:
 * 
 * <pre>
 *  ILLUSTRATIVE EXAMPLE MARKOV CHAIN:
 *  startTokens: { "a":2 }
 *  bigramFrequencies:
 *  "!":    { "and":1 }
 *  "?":    { "&lt;END&gt;":1 }
 *  "a":    { "banana":2  "chair":1  "table":1 }
 *  "and":  { "a":2 }
 *  "banana":   { "!":1  "?":1 }
 *  "chair":    { "&lt;END&gt;":1 }
 *  "table":    { "and":1 }
 * </pre>
 * <p>
 * For this training data, because both tweets start with "a", the
 * {@code startTokens}
 * distribution simply records that fact.
 * <p>
 * The {@code bigramFrequencies} data structure records the information about
 * occurrences of adjacent tokens in the tweets. For instance,
 * the token "a" is followed by "banana" twice, "chair" once, and "table" once.
 * The token "!" is followed by "and" once, whereas "?" (like "chair") appears
 * only at the end of a tweet.
 *
 * <p>
 * NOTE: we use the {@code END_TOKEN} marker {@code "<END>"} to mark the end of
 * a tweet.
 *
 * <p>
 * <b>SAMPLING</b>
 * <p>
 * Once we have trained the Markov Chain, we can use it to generate new
 * sequences that
 * mimic the training inputs. This is done by conducting a "random" walk through
 * the chain.
 * <p>
 * The walk begins by choosing a token from the {@code startTokens}
 * distribution.
 * In the running example, since both tweets start with the token "a", the only
 * possible starting token is "a".
 * <p>
 * Then, a sequence of tokens is generated by choosing the next symbol according
 * to
 * the bigram distributions until the {@code END_TOKEN} is encountered, at which
 * point the walk
 * is finished. For example. after the token "a", the walk might pick "chair",
 * and then,
 * because "chair" is always followed by the {@code END_TOKEN}, the walk is
 * complete. A different
 * walk might yield the sequence "a table and a banana?"
 * <p>
 * We model this random walk process as an {@code Iterator} that, given a source
 * of
 * (random) numbers, yields the sequence of tokens visited by choosing that
 * "path" through
 * the Markov Chain, as explained in more detail below.
 * <p>
 *
 * Your job is to complete the code in this class to provide the functionality
 * described above.
 *
 */
public class MarkovChain {

    /** probability distribution of initial tokens in a sentence */
    final ProbabilityDistribution<String> startTokens;

    /** for each token, probability distribution of next token in a sentence */
    final Map<String, ProbabilityDistribution<String>> bigramFrequencies;

    /** end of sentence marker */
    static final String END_TOKEN = "<END>";

    /**
     * Construct an empty {@code MarkovChain} that can later be trained.
     *
     * This constructor is implemented for you.
     */
    public MarkovChain() {
        this.bigramFrequencies = new TreeMap<>();
        this.startTokens = new ProbabilityDistribution<>();
    }

    /**
     * Construct a trained {@code MarkovChain} from the given list of training data.
     * The training data is assumed to be non-null. Uses the {@link #addSequence}
     * method
     * on each of the provided sequences. (It is recommended that you implement
     * {@link #addBigram} and {@link #addSequence} first.)
     *
     * @param trainingData - the input sequences of tokens from which to construct
     *                     the {@code MarkovChain}
     */
    public MarkovChain(List<List<String>> trainingData) {
        this.bigramFrequencies = new TreeMap<>();
        this.startTokens = new ProbabilityDistribution<>();
        // TODO: complete this constructor
        for (List<String> sequence : trainingData) {
            addSequence(sequence.iterator());
        }
    }

    /**
     * Adds a bigram to the Markov Chain information by
     * recording it in the appropriate probability distribution
     * of {@code bigramFrequencies}. (If this is the first time that {@code first}
     * has appeared in a bigram, creates a new probability distribution first.)
     * You may assume that the two Strings in the bigram are tokens
     * that have been produced by TweetParser.
     *
     * @param first  The first token of the Bigram (should not be null)
     * @param second The second token of the Bigram (should not be null)
     * @throws IllegalArgumentException - when either parameter is null
     */
    void addBigram(String first, String second) {
        // TODO: Complete this method.
        if (first == null || second == null) {
            throw new IllegalArgumentException("Neither token can be null");
        }
        //ProbabilityDistribution<String> distribution = bigramFrequencies.computeIfAbsent(
                //first, k -> new ProbabilityDistribution<>());
        ProbabilityDistribution distribution;
        if (bigramFrequencies.containsKey(first)) {
            distribution = bigramFrequencies.get(first);
        } else {
            distribution = new ProbabilityDistribution();
            bigramFrequencies.put(first, distribution);
        }
        distribution.record(second);
    }

    /**
     * Adds a single tweet's training data to the Markov Chain frequency
     * information, by:
     *
     * <ol>
     * <li>recording the first token in {@code startTokens}
     * <li>recording each subsequent bigram of co-occurring pairs of tokens
     * <li>recording a final bigram of the last token of the {@code tweet} and
     * {@code END_TOKEN} to mark the end of the sequence
     * </ol>
     * <p>
     * Does nothing if the tweet is empty.
     *
     * <p>
     * Note that this is where you <i>train</i> the MarkovChain.
     *
     * @param tweet an iterator representing one tweet of training data
     * @throws IllegalArgumentException when the tweet Iterator is null
     */
    public void addSequence(Iterator<String> tweet) {
        // TODO: Complete this method.
        if (tweet == null) {
            throw new IllegalArgumentException("Tweet iterator is null");
        }
        if (!tweet.hasNext()) {
            return;
        }
        String token = tweet.next();
        startTokens.record(token);
        while (tweet.hasNext()) {
            String nextToken = tweet.next();
            addBigram(token, nextToken);
            token = nextToken;
        }
        addBigram(token, END_TOKEN);
    }

    /**
     * Returns the ProbabilityDistribution for a given token. Returns null if
     * none exists. This function is implemented for you.
     *
     * @param token - the token for which the ProbabilityDistribution is sought
     * @throws IllegalArgumentException - when parameter is null
     * @return a ProbabilityDistribution
     */
    ProbabilityDistribution<String> get(String token) {
        if (token == null) {
            throw new IllegalArgumentException("token cannot be null.");
        }
        return bigramFrequencies.get(token);
    }

    /**
     * Gets a walk through the Markov Chain that follows
     * the path given by the {@code NumberGenerator}. See
     * {@link MarkovChainIterator} for the details.
     *
     * This function is implemented for you.
     *
     * @param ng the path to follow (represented as a {@code NumberGenerator},
     *           assumed nonnull)
     * @return an {@code Iterator} that yields the tokens on that path
     *
     */
    public Iterator<String> getWalk(NumberGenerator ng) {
        return new MarkovChainIterator(startTokens, bigramFrequencies, ng);
    }

    /**
     * Gets a random walk through the Markov Chain.
     *
     * This function is implemented for you.
     *
     * @return an {@code Iterator} that yields the tokens on that path
     */
    public Iterator<String> getRandomWalk() {
        return getWalk(new RandomNumberGenerator());
    }

    /**
     * Generate a list of numbers such that if it is installed as the
     * number generator for the MarkovChain, and used as an iterator,
     * the tokens returned in sequence will be the list of provided tokens.
     *
     * Note that the length of the list of numbers is equal to the length
     * of the list of tokens plus one (for the {@code END_TOKEN}).
     *
     * This function is implemented for you.
     *
     * @param tokens an ordered list of tokens that the MarkovChain should generate
     *
     * @return a list of integers representing a walk through the Markov Chain that
     *         produces the given sequence of tokens
     *
     * @throws IllegalArgumentException when any of the following are true:
     *                                  <ul>
     *                                  <li>{@code tokens} is null or empty
     *                                  <li>the first token in the list is not in
     *                                  {@code startTokens}
     *                                  <li>any of the tokens in the list is not
     *                                  found as a key in the chain
     *                                  <li>if the last token of the list cannot
     *                                  transition to {@code END_TOKEN}
     *                                  </ul>
     */
    public List<Integer> findWalkChoices(List<String> tokens) {
        if (tokens == null || tokens.isEmpty()) {
            throw new IllegalArgumentException("Invalid empty or null tokens");
        }
        tokens.add(MarkovChain.END_TOKEN);
        List<Integer> choices = new LinkedList<>();

        String curToken = tokens.remove(0);
        choices.add(startTokens.index(curToken));

        while (tokens.size() > 0) {
            ProbabilityDistribution<String> curDist = bigramFrequencies.get(curToken);
            String nextToken = tokens.remove(0);
            choices.add(curDist.index(nextToken));
            curToken = nextToken;
        }
        return choices;
    }

    /**
     * Use this method to print out markov chains with tokens and probability
     * distributions.
     *
     * This function is implemented for you.
     */
    @Override
    public String toString() {
        StringBuilder res = new StringBuilder();
        res.append("startTokens: ").append(startTokens.toString());
        res.append("\nbigramFrequencies:\n");
        for (Map.Entry<String, ProbabilityDistribution<String>> c : bigramFrequencies.entrySet()) {
            res.append("\"");
            res.append(c.getKey());
            res.append("\":\t");
            res.append(c.getValue().toString());
            res.append("\n");
        }
        return res.toString();
    }
}
